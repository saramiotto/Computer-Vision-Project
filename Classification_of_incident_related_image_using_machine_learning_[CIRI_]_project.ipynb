{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hG-6AYjPXk5a"
      },
      "source": [
        "#**Classification of incident-related image using machine learning [CIRI ]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbfTUZ7VYKwD"
      },
      "source": [
        "**A bit of context...**\n",
        "\n",
        "In today's world, computer vision plays a crucial role in the automatic recognition and classification of visual inputs such as images and videos, enabling humans to extract meaningful information from them. This information is then utilized to make decisions or provide recommendations. As the objective of creating artificial intelligence systems based on learning models and visual inputs aligns with human goals, machine learning and computer vision have become closely intertwined. The ultimate aim of these AI systems is to gain an understanding of the world that parallels that of humans.\n",
        "\n",
        "One field where computer vision can have a significant impact is in relief organizations. These organizations often need to quickly acquire and process information to respond to natural disasters and other events that require human intervention. Currently, the analysis of such situations relies on manual processing, which is time-consuming and inefficient. To address this issue, the computer science community has focused on analyzing satellite imagery, remote sensing data, and other sources to automate data processing.\n",
        "\n",
        "In addition, people sharing posts on social media during or after a natural disaster can provide a new source of information, including time-stamps, images, textual descriptions, and audio recordings. Recently, new large-scale datasets have been made available for incident recognition in natural settings. These datasets aim to serve as a foundation for training algorithms to automatically filter relevant images. The identification of these images can help relief organizations better organize their response to the event.\n",
        "\n",
        "The goal of the project is to implement a classification framework for the purpose of incident recognition in images. Different typologies are explored and compared. The use of computer vision in this context can greatly enhance the speed and efficiency of data processing, thereby enabling relief organizations to respond more effectively to natural disasters and other events that require immediate intervention."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHnEDS2dXtGW"
      },
      "source": [
        "## **Importing libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hWIchPToi_W"
      },
      "outputs": [],
      "source": [
        "!pip install -q keras\n",
        "!pip install tensorflow\n",
        "!pip install torchviz\n",
        "import keras\n",
        "from keras.layers import Input, Lambda, Dense, Flatten\n",
        "from keras.models import Model\n",
        "from keras.applications.vgg19 import VGG19\n",
        "from keras.applications.vgg16 import preprocess_input\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "import numpy as np\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "from os import makedirs\n",
        "from tqdm import tqdm\n",
        "from google.colab import drive\n",
        "from os import listdir\n",
        "from os.path import join\n",
        "from shutil import copytree\n",
        "from __future__ import print_function, division\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "import pandas as pd\n",
        "from skimage import io, transform\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset, ConcatDataset\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "import torchvision\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize, Resize\n",
        "from torchvision import datasets, models, transforms, utils\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.io import read_image\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "import seaborn as sns\n",
        "from collections import Counter \n",
        "from PIL import Image, UnidentifiedImageError\n",
        "import torchviz\n",
        "from torchviz import make_dot\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "import time\n",
        "import copy\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nimn2SXYXytC"
      },
      "source": [
        "Mounting the drive containing the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_bDqbXTqLaj"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdiVrF8abDvq"
      },
      "source": [
        "## **Dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Is_od4MZcET3"
      },
      "source": [
        "The dataset used is the so-called Incidents1M. It contains images describing different topologies of natural disasters, incidents or damages. In total the images are 977088, with 43 accident types and 49 locations where these accidents took place. However, in this code we have used a subset of it with 1000 samples per category. \n",
        "\n",
        "Here below that Dataset is loaded and pre-processed. As can be seen, the images are resized and normalized. To better handle it, Pytorch's Dataset and Dataloader classes are generated and divided into training, validation and test set, according to a percentage we arbitrarily defined, 80%, 10% and 10% respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k179LIRuK3qC"
      },
      "outputs": [],
      "source": [
        "# Define a sequence of image transformations to be applied to the input image\n",
        "transform = transforms.Compose([\n",
        "    # Resize the image to 300x300 pixels\n",
        "    transforms.Resize((300, 300)),\n",
        "    # Convert the image to a PyTorch tensor\n",
        "    transforms.ToTensor(),\n",
        "    # Normalize the pixel values of the image using the specified mean and standard deviation\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# Define the path to the directory containing the dataset\n",
        "data_path = '/content/drive/My Drive/Incidents-subset/'\n",
        "\n",
        "# Keep only files with certain extensions\n",
        "extensions = ['.jpg', '.jpeg', '.JPG', '.JPEG']\n",
        "files = [f for f in os.listdir(data_path) if any(f.endswith(ext) for ext in extensions)]\n",
        "\n",
        "# Create a list to store the valid images\n",
        "valid_files = []\n",
        "\n",
        "# Iterate over the files and check if they are valid images\n",
        "for file in files:\n",
        "    try:\n",
        "        Image.open(os.path.join(data_path, file))\n",
        "        valid_files.append(file)\n",
        "    except (UnidentifiedImageError, OSError):\n",
        "        print(f'Skipping file {file} as it is not a valid image file')\n",
        "\n",
        "def is_valid_file(path):\n",
        "  try:\n",
        "    Image.open(path)\n",
        "    return True\n",
        "  except:\n",
        "    return False\n",
        "\n",
        "# Create the dataset from the valid image files using the ImageFolder class\n",
        "dataset = datasets.ImageFolder(root=data_path, transform=transform, loader=lambda x: Image.open(x).convert('RGB'), is_valid_file=is_valid_file)\n",
        "\n",
        "# Get the size of the dataset\n",
        "dataset_size = len(dataset)\n",
        "\n",
        "# Set the size of the training set to 80% of the dataset\n",
        "train_size = int(dataset_size * 0.8)\n",
        "\n",
        "# Set the size of the validation set to 10% of the dataset\n",
        "valid_size = int(dataset_size * 0.1)\n",
        "\n",
        "# Set the size of the test set to the remaining 10%\n",
        "test_size = dataset_size - train_size - valid_size\n",
        "\n",
        "# Generate random indices for the training set\n",
        "train_indices = random.sample(range(dataset_size), train_size)\n",
        "\n",
        "# Generate random indices for the validation set\n",
        "valid_indices = random.sample(set(range(dataset_size)) - set(train_indices), valid_size)\n",
        "\n",
        "# Generate the remaining indices for the test set\n",
        "test_indices = list(set(range(dataset_size)) - set(train_indices) - set(valid_indices))\n",
        "\n",
        "# Create Subset objects for the training, validation, and test sets\n",
        "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
        "valid_dataset = torch.utils.data.Subset(dataset, valid_indices)\n",
        "test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
        "\n",
        "# Create dataloaders for each dataset\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=16, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoTwlzXieSQZ"
      },
      "source": [
        "Here the list of the labels belonging to the different categories is provided:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XrO5SRibnJjl"
      },
      "outputs": [],
      "source": [
        "classes = [\"airplane accident\", \"bicycle accident\", \"car accident\", \"collapsed\", \"earthquake\", \"flooded\", \"ice storm\", \"nuclear explosion\", \"oil spill\",\n",
        "           \"tornado\", \"volcanic eruption\", \"wildfire\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Tk_00Sbeic4"
      },
      "source": [
        "## **Some statistics about the dataset**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrhZslRueojL"
      },
      "source": [
        "The statistics show that our dataset contains 4734 total samples distributed as follows:\n",
        "\n",
        "*   Class 0 : 569\n",
        "*   Class 1 : 228\n",
        "*   Class 2 : 249\n",
        "*   Class 3 : 497\n",
        "*   Class 4 : 393\n",
        "*   Class 5 : 708\n",
        "*   Class 6 : 615\n",
        "*   Class 7 : 231\n",
        "*   Class 8 : 248\n",
        "*   Class 9 : 249\n",
        "*   Class 10 : 498\n",
        "*   Class 11 : 249\n",
        "\n",
        "Therefore the dataser appears to be unbalanced.\n",
        "\n",
        "In the output besides the statistics, also one random image per class is printed:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWzfl1YJDLNc"
      },
      "outputs": [],
      "source": [
        "# Count the number of samples per class\n",
        "class_counts = {}\n",
        "image_sizes = []\n",
        "for i, (image_tensor, label) in enumerate(dataset):\n",
        "    if label in class_counts:\n",
        "        class_counts[label] += 1\n",
        "    else:\n",
        "        # Print one image per label\n",
        "        file_name, label = dataset.samples[i]\n",
        "        import cv2\n",
        "        from google.colab.patches import cv2_imshow\n",
        "        print(f\"Image for label {classes[label]}({label}):\")\n",
        "        print(f\"Filename: {file_name}\")\n",
        "        img = cv2.imread(file_name, cv2.IMREAD_UNCHANGED)\n",
        "        cv2_imshow(img)\n",
        "\n",
        "        class_counts[label] = 1\n",
        "\n",
        "    # Get the size of the image tensor\n",
        "    size = image_tensor.shape[-2:]\n",
        "\n",
        "    # Get the number of channels in the image tensor\n",
        "    if image_tensor.shape[-1] == 1:\n",
        "        channels = 'L'\n",
        "    else:\n",
        "        channels = 'RGB'\n",
        "\n",
        "    # Add the size and number of channels to the list\n",
        "    image_sizes.append((size[0], size[1], channels))\n",
        "\n",
        "# Print the number of samples and classes in the dataset\n",
        "print(f'Number of samples: {len(dataset)}')\n",
        "print(f'Number of classes: {len(class_counts)}')\n",
        "\n",
        "# Print the number of samples per class\n",
        "print('Samples per class:')\n",
        "for label, count in class_counts.items():\n",
        "    print(f'Class {label}: {count}')\n",
        "\n",
        "# Print the number of samples per class in a table format\n",
        "print('\\nSamples per class (table format):')\n",
        "print('---------------------------------')\n",
        "print('| Class | Samples | Percentage |')\n",
        "print('---------------------------------')\n",
        "total_samples = len(dataset)\n",
        "for label, count in class_counts.items():\n",
        "    percentage = count / total_samples * 100\n",
        "    print(f'| {label:5d} | {count:7d} | {percentage:9.2f}% |')\n",
        "print('---------------------------------')\n",
        "\n",
        "# Calculate and print the minimum, maximum, and average image dimensions\n",
        "min_width, min_height, min_channels = min(image_sizes)\n",
        "max_width, max_height, max_channels = max(image_sizes)\n",
        "avg_width = sum([size[0] for size in image_sizes]) / len(image_sizes)\n",
        "avg_height = sum([size[1] for size in image_sizes]) / len(image_sizes)\n",
        "avg_channels = sum([1 if size[2] == 'L' else 3 for size in image_sizes]) / len(image_sizes)\n",
        "\n",
        "print('Image dimensions:')\n",
        "print(f'Minimum width: {min_width}')\n",
        "print(f'Minimum height: {min_height}')\n",
        "print(f'Minimum channels: {min_channels}')\n",
        "print(f'Maximum width: {max_width}')\n",
        "print(f'Maximum height: {max_height}')\n",
        "print(f'Maximum channels: {max_channels}')\n",
        "print(f'Average width: {avg_width:.2f}')\n",
        "print(f'Average height: {avg_height:.2f}')\n",
        "print(f'Average channels: {avg_channels:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first approach we adopt in order to classify our images is the popular k-NN"
      ],
      "metadata": {
        "id": "rl6tiCI_Tnte"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvoboEUl199_"
      },
      "source": [
        "# **K-nearest neighbors**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LAuLVcyY10e-"
      },
      "source": [
        "A straightforward, non-parametric machine learning approach called K-nearest neighbors is employed for both classification and regression. K-fundamental NN's premise is to forecast a new data point's label based on the labels of its k closest neighbors in the training set.\n",
        "\n",
        "The k-NN technique first determines the distances between each new data point and every other point in the training set before making a prediction for a new data point. The new point is then given the label that is most prevalent among its k closest neighbors after being chosen from the k points with the least distances.\n",
        "\n",
        "A hyperparameter that may be adjusted to improve the algorithm's performance is the value of k. The algorithm will be more sensitive to regional changes in the data if k is set to a lower value, but there is a chance that it will become overfitted. A higher number of k will strengthen the algorithm's resistance to data noise, but it may also result in underfitting.\n",
        "\n",
        "\n",
        "The k-NN algorithm's readability and simplicity are two benefits. It is simple to comprehend how the algorithm uses the separations between data points to produce predictions. Moreover, k-NN is a great tool for examining complex and nonlinear interactions because it does not demand assumptions about the data's underlying distribution.\n",
        "\n",
        "The k-NN approach, however, can be computationally expensive as well, particularly when dealing with huge datasets. Furthermore, the choice of distance measure and the existence of redundant or irrelevant information in the data may affect how well the algorithm performs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a range of k values to try\n",
        "k_values = [1, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
        "\n",
        "for k in k_values:\n",
        "    print(f\"Trying k={k}...\")\n",
        "    # Creating a k-NN model\n",
        "    knn_model = KNeighborsClassifier(n_neighbors=k)\n",
        "\n",
        "    # Extract the features and labels for the training set\n",
        "    train_features = []\n",
        "    train_labels = []\n",
        "    for batch in train_loader:\n",
        "        features, labels = batch\n",
        "        features = features.numpy().reshape(len(features), -1)\n",
        "        labels = labels.numpy()\n",
        "        train_features.append(features)\n",
        "        train_labels.append(labels)\n",
        "    train_features = np.concatenate(train_features)\n",
        "    train_labels = np.concatenate(train_labels)\n",
        "\n",
        "    # Extract the features and labels for the test set\n",
        "    test_features = []\n",
        "    test_labels = []\n",
        "    for batch in test_loader:\n",
        "        features, labels = batch\n",
        "        features = features.numpy().reshape(len(features), -1)\n",
        "        labels = labels.numpy()\n",
        "        test_features.append(features)\n",
        "        test_labels.append(labels)\n",
        "    test_features = np.concatenate(test_features)\n",
        "    test_labels = np.concatenate(test_labels)\n",
        "\n",
        "    knn_model.fit(train_features, train_labels)\n",
        "\n",
        "    # Make predictions on the test set\n",
        "    y_pred = knn_model.predict(test_features)\n",
        "\n",
        "    # Compute the evaluation metrics\n",
        "    accuracy = accuracy_score(test_labels, y_pred)\n",
        "    precision = precision_score(test_labels, y_pred, average='weighted')\n",
        "    recall = recall_score(test_labels, y_pred, average='weighted')\n",
        "    f1 = f1_score(test_labels, y_pred, average='weighted')\n",
        "\n",
        "    # Print the evaluation metrics\n",
        "    print('Accuracy:', accuracy)\n",
        "    print('Precision:', precision)\n",
        "    print('Recall:', recall)\n",
        "    print('F1 score:', f1)\n",
        "\n",
        "   \n",
        "\n",
        "    # Compute the accuracy of the model\n",
        "    accuracy = knn_model.score(test_features, test_labels)\n",
        "\n",
        "    # Print the accuracy gained\n",
        "    print(f'Accuracy for k={k}: {accuracy}')\n"
      ],
      "metadata": {
        "id": "z4RHV4yzfdgI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Deep learning approach**"
      ],
      "metadata": {
        "id": "4HLCEQChT81J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the quest to achieve high performance in classification tasks, it is not uncommon to encounter situations where traditional machine learning algorithms such as k-Nearest Neighbors (k-NN) may not yield satisfactory results. In such cases, deep learning has emerged as a popular alternative due to its ability to automatically extract relevant features from raw data, leading to improved accuracy in classification tasks.\n",
        "\n",
        "We would then use a validation dataset to evaluate the performance of the different frameworks and identify the best performing one. The winning framework would then be further evaluated on a test dataset to ensure that it performs well in real-world scenarios.\n",
        "\n",
        "This process of creating and evaluating multiple deep learning frameworks can be time-consuming and resource-intensive, but it can yield significant performance improvements in classification tasks. Additionally, it can lead to a better understanding of the dataset, as we can investigate which features the deep learning models are learning and what patterns they are capturing.\n",
        "\n",
        "Overall, the decision to switch from traditional machine learning algorithms to deep learning can be a wise choice in situations where performance is not satisfactory"
      ],
      "metadata": {
        "id": "ucf-4f_DUElO"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpqpotEGgArD"
      },
      "source": [
        "# **Neural Network frameworks**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LiPw8iATX22p"
      },
      "source": [
        "## **ResNet18 from the scratch**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yx_HZWd6gG1g"
      },
      "source": [
        "The first framework used for the purpose of image classification is ResNet 18, here implemented from the scratch.\n",
        "\n",
        "ResNet18 is a convolutional neural network architecture part of the ResNet family of models, which was designed to overcome the problem of vanishing gradients in very deep neural networks. The vanishing gradient problem occurs when gradients become very small during backpropagation, making it difficult to train deep neural networks.\n",
        "\n",
        "ResNet18 is made up of 18 layers, including several residual blocks. Residual blocks are a key feature of the ResNet architecture and are designed to enable the efficient training of very deep neural networks. In a residual block, the input to a layer is added to the output of the layer after passing through one or more convolutional layers. This so-called skip connection enables the network to learn residual functions, which can be easier to optimize than the original functions.\n",
        "\n",
        "The techniques of Droupout as well as L2-normalization and batch normalization are added as regularization techniques in order to prevent the overfitting of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VnjJGYjzEAd8"
      },
      "outputs": [],
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    \n",
        "    def __init__(self, inchannel, outchannel, stride=1):\n",
        "    \n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        # First convolutional layer\n",
        "        self.conv1 = nn.Conv2d(inchannel, outchannel, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(outchannel)\n",
        "\n",
        "        # Second convolutional layer\n",
        "        self.conv2 = nn.Conv2d(outchannel, outchannel, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(outchannel)\n",
        "\n",
        "        # Shortcut connection to add to output\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or inchannel != outchannel:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(inchannel, outchannel, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(outchannel)\n",
        "            )\n",
        "            \n",
        "    def forward(self, x):\n",
        "\n",
        "        # First convolutional layer\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "\n",
        "        # Second convolutional layer\n",
        "        out = self.bn2(self.conv2(out))\n",
        "\n",
        "        # Shortcut connection\n",
        "        out = out + self.shortcut(x)\n",
        "\n",
        "        # ReLU activation\n",
        "        out = F.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    \n",
        "    def __init__(self, ResidualBlock, num_classes=12):\n",
        "        super(ResNet, self).__init__()\n",
        "    \n",
        "        self.inchannel = 6\n",
        "      \n",
        "        self.conv1 = nn.Sequential(\n",
        "            # define first convolution layer\n",
        "            \n",
        "            nn.Conv2d(3, 6, kernel_size=(2, 2), stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(6),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        # define the first residual block layer\n",
        "        self.layer1 = self.make_layer(ResidualBlock, 32, 2, stride=1)  \n",
        "\n",
        "        # define the second residual block layer\n",
        "        self.layer2 = self.make_layer(ResidualBlock, 64, 2, stride=2)  \n",
        "\n",
        "        # define the third residual block layer\n",
        "        self.layer3 = self.make_layer(ResidualBlock, 128, 2, stride=2)  \n",
        "\n",
        "        # define the fourth residual block layer\n",
        "        self.layer4 = self.make_layer(ResidualBlock, 512, 2, stride=2)\n",
        "\n",
        "        # define global average pooling layer\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # define the fully connected layer\n",
        "        self.fc = nn.Linear(512, num_classes)\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "\n",
        "        \n",
        "    def make_layer(self, block, channels, num_blocks, stride):\n",
        "      \n",
        "        strides = [stride] + [1] * (num_blocks - 1)\n",
        "      \n",
        "        layers = []\n",
        "      \n",
        "        for stride in strides:\n",
        "      \n",
        "            # create a residual block layer and append it to layers\n",
        "            layers.append(block(self.inchannel, channels, stride))  \n",
        "            self.inchannel = channels\n",
        "      \n",
        "        # create a sequential layer using the created residual blocks\n",
        "        return nn.Sequential(*layers)  \n",
        "    \n",
        "    def forward(self, x):\n",
        "      \n",
        "        # pass the input through the first convolution layer\n",
        "        out = self.conv1(x)  \n",
        "        \n",
        "        # pass the input through the first residual block layer\n",
        "        out = self.layer1(out) \n",
        "        \n",
        "        # pass the input through the second residual block layer\n",
        "        out = self.layer2(out)  \n",
        "        \n",
        "        # pass the input through the third residual block layer\n",
        "        out = self.layer3(out)  \n",
        "        \n",
        "        # pass the input through the fourth residual block layer\n",
        "        out = self.layer4(out)  \n",
        "        \n",
        "        # perform global average pooling on the output of the last residual block layer\n",
        "        out = self.global_avg_pool(out)\n",
        "        \n",
        "        # flatten the output tensor\n",
        "        out = out.view(out.size(0), -1)\n",
        "        \n",
        "        # L2 normalization\n",
        "        out = F.normalize(out, p=2, dim=1, eps=1e-12)\n",
        "        \n",
        "        # Dropout\n",
        "        out = self.dropout(out)\n",
        "        \n",
        "        # pass the output tensor through the fully connected layer\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "def ResNet18():\n",
        "  return ResNet(ResidualBlock)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3na1coO_r7V7"
      },
      "source": [
        "## **Transfer Learning for Resnet, Alexnet, VGG, Squeezenet, and Densenet**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2uZu30GYOXJ"
      },
      "source": [
        "In the following code cell, we aim to evaluate different frameworks that have been imported directly from the PyTorch library, namely AlexNet, VGG, SqueezeNet, and Densenet.\n",
        "\n",
        "AlexNet is a deep convolutional neural network architecture that was proposed in 2012 by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. This architecture has a total of 8 layers, including 5 convolutional layers, 2 fully connected layers, and 1 softmax layer. Some of its most noteworthy features are:\n",
        "\n",
        "*   The rectified linear unit (ReLU) activation function is utilized, which enhances training time and reduces the risk of the vanishing gradient problem.\n",
        "*   The architecture introduced the concept of overlapping pooling, which implies that the pooling regions of adjacent pooling layers overlap instead of being disjoint.\n",
        "*   Local response normalization (LRN) is also included in the architecture, which aids in normalizing the response of adjacent neurons and enhances the model's generalization.\n",
        "*   AlexNet was trained on the ImageNet dataset, which comprises millions of labeled images across 1,000 different categories.\n",
        "\n",
        "\n",
        "VGG (Visual Geometry Group) is a convolutional neural network architecture proposed in 2014 by Karen Simonyan and Andrew Zisserman from the University of Oxford. VGG is renowned for its simplicity and effectiveness in deep learning for computer vision tasks. Some key features of VGG are:\n",
        "\n",
        "*   VGG consists of a total of 19 layers, including 16 convolutional layers and 3 fully connected layers.\n",
        "*   The convolutional layers in VGG are formed by 3x3 filters, which are stacked on top of each other to create deeper representations.\n",
        "*   The pooling layers in VGG use max pooling with a 2x2 filter size and a stride of 2.\n",
        "*   VGG employs the rectified linear unit (ReLU) activation function, which improves training time and reduces the risk of the vanishing gradient problem.\n",
        "*   Like AlexNet, VGG was trained on the ImageNet dataset.\n",
        "\n",
        "\n",
        "SqueezeNet is a convolutional neural network architecture introduced in 2016 by Forrest N. Iandola, Song Han, Matthew W. Moskewicz, Khalid Ashraf, William J. Dally, and Kurt Keutzer from the University of California, Berkeley. SqueezeNet is renowned for its small size, which makes it ideal for resource-constrained environments like mobile devices and embedded systems. Here are some key features of SqueezeNet:\n",
        "\n",
        "*   SqueezeNet is a small network with a total of only 23 layers, which is significantly smaller than other popular architectures such as AlexNet and VGG.\n",
        "*   It achieves its small size by employing a combination of 1x1 convolutional filters and a technique called \"fire modules,\" which are designed to balance the number of filters and the computational cost.\n",
        "*   SqueezeNet also uses a technique called \"deep compression,\" which further reduces the size of the network by applying pruning and quantization techniques. Despite its small size, SqueezeNet achieves competitive accuracy on the ImageNet dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5vO7TrGdNGS"
      },
      "source": [
        "The following function is to use the pre-trained model only to extract image features. This can be useful to save computational time and resources, as there is no need to calculate gradients and update model weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IT3e7yWgdP05"
      },
      "outputs": [],
      "source": [
        "def set_parameter_requires_grad(model, feature_extracting):\n",
        "    \n",
        "    \"\"\"\n",
        "    A function that sets the requires_grad attribute of model parameters\n",
        "    to False if feature_extracting is True, and to True otherwise.\n",
        "\n",
        "    Args:\n",
        "    - model: a PyTorch model\n",
        "    - feature_extracting: a boolean value that indicates whether we want\n",
        "      to extract features from the model (True) or fine-tune the model (False)\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "\n",
        "    # If we're only extracting features, we don't want to update the model parameters\n",
        "    if feature_extracting:\n",
        "        # Loop over all parameters in the model and set their requires_grad attribute to False\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vItWJoMZH4p_"
      },
      "source": [
        "## **Set the model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GSHesQAdjsX"
      },
      "source": [
        "Here below we can choose which one between the neural networks architectures previously mentioned we want to test, just changing the parameter \"model_name\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yZTI0aU-jtFF"
      },
      "outputs": [],
      "source": [
        "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet,ResNet18]\n",
        "model_name = \"alexnet\"\n",
        "\n",
        "# Number of classes in the dataset\n",
        "num_classes = 12\n",
        "\n",
        "# Batch size for training (change depending on how much memory you have)\n",
        "batch_size = 32\n",
        "\n",
        "# Number of epochs to train for\n",
        "num_epochs = 50\n",
        "\n",
        "# Flag for feature extracting. When False, we finetune the whole model,\n",
        "#   when True we only update the reshaped layer params\n",
        "feature_extract = True\n",
        "\n",
        "def initialize_model(model_name, num_classes, feature_extract, use_pretrained):\n",
        "    # Initialize these variables which will be set in this if statement. Each of these\n",
        "    #   variables is model specific.\n",
        "    model_ft = None\n",
        "    input_size = 0\n",
        "\n",
        "    if model_name == \"resnet\":\n",
        "        \"\"\" Resnet18\n",
        "        \"\"\"\n",
        "        model_ft = models.resnet18(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.fc.in_features\n",
        "        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"alexnet\":\n",
        "        \"\"\" Alexnet\n",
        "        \"\"\"\n",
        "        model_ft = models.alexnet(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"vgg\":\n",
        "        \"\"\" VGG11_bn\n",
        "        \"\"\"\n",
        "        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier[6].in_features\n",
        "        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"squeezenet\":\n",
        "        \"\"\" Squeezenet\n",
        "        \"\"\"\n",
        "        model_ft = models.squeezenet1_0(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        model_ft.classifier[1] = nn.Conv2d(512, num_classes, kernel_size=(1,1), stride=(1,1))\n",
        "        model_ft.num_classes = num_classes\n",
        "        input_size = 224\n",
        "\n",
        "    elif model_name == \"densenet\":\n",
        "        \"\"\" Densenet\n",
        "        \"\"\"\n",
        "        model_ft = models.densenet121(pretrained=use_pretrained)\n",
        "        set_parameter_requires_grad(model_ft, feature_extract)\n",
        "        num_ftrs = model_ft.classifier.in_features\n",
        "        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n",
        "        input_size = 224\n",
        "    \n",
        "    elif model_name == \"ResNet18\":\n",
        "        model_ft = ResNet18()\n",
        "        input_size = 224\n",
        "    else:\n",
        "        print(\"Invalid model name, exiting...\")\n",
        "        exit()\n",
        "\n",
        "    return model_ft, input_size\n",
        "\n",
        "# Initialize the model for this run\n",
        "model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=False)\n",
        "\n",
        "# Print the model we just instantiated\n",
        "print(model_ft)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QGPsnIjfqmx"
      },
      "source": [
        "This function is used to define which model parameters will be updated during training, based on the choice of \"feature extract\" and the finetuning request.\n",
        "\n",
        "Specifically, the function retrieves all the model parameters \"model_ft\" and assigns them to the variable \"params_to_update\". If \"feature_extract\" is set to True, then the function extracts only those parameters that have the \"requires_grad\" attribute set to True and adds them to the \"params_to_update\" list. These will be the only parameters that will be updated during training. On the other hand, if \"feature_extract\" is set to False, all the parameters in the model will have the \"requires_grad\" attribute set to True and will be updated during training.\n",
        "\n",
        "Finally, the function prints the names of the parameters that will be updated during training (if \"feature_extract\" is False) or that have been selected for update (if \"feature_extract\" is True). This information can be useful for verifying that model parameters are updated correctly during training and for monitoring the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0BJJppS0eMh6"
      },
      "outputs": [],
      "source": [
        "# Gather the parameters to be optimized/updated in this run. If we are finetuning we will be updating all parameters. However, if we are doing \n",
        "# feature extract method, we will only update the parameters that we have just initialized, i.e. the parameters with requires_grad is True\n",
        "\n",
        "def update_params_to_learn(model_ft, feature_extract):\n",
        "    \n",
        "    \"\"\"\n",
        "    A function that determines which parameters of a PyTorch model should be\n",
        "    updated during training, based on whether we want to fine-tune the model\n",
        "    (feature_extract = False) or extract features (feature_extract = True).\n",
        "\n",
        "    Args:\n",
        "    - model_ft: a PyTorch model\n",
        "    - feature_extract: a boolean value that indicates whether we want to\n",
        "      extract features from the model (True) or fine-tune the model (False)\n",
        "\n",
        "    Returns:\n",
        "    - params_to_update: a list of parameters to be updated during training\n",
        "    \"\"\"\n",
        "\n",
        "    # Get all parameters of the model\n",
        "    params_to_update = model_ft.parameters()\n",
        "\n",
        "    # If we're only extracting features, we only want to update certain parameters\n",
        "    if feature_extract:\n",
        "        # Create an empty list to store the parameters we want to update\n",
        "        params_to_update = []\n",
        "        # Loop over all named parameters in the model and check if requires_grad is True\n",
        "        for name, param in model_ft.named_parameters():\n",
        "            if param.requires_grad == True:\n",
        "                # If the parameter should be updated, append it to the list\n",
        "                params_to_update.append(param)\n",
        "                print(\"\\t\", name)\n",
        "    # If we're fine-tuning the model, update all parameters with requires_grad=True\n",
        "    else:\n",
        "        for name, param in model_ft.named_parameters():\n",
        "            if param.requires_grad == True:\n",
        "                print(\"\\t\", name)\n",
        "\n",
        "    return params_to_update"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8EBSIrqgOEC"
      },
      "source": [
        "## **Print a png image of the framework concerned**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4sM_XpYxp6MW"
      },
      "outputs": [],
      "source": [
        "# import the ResNet architecture and the ResidualBlock module\n",
        "model = model_ft\n",
        "\n",
        "# create a sample input tensor with shape (1, 3, 224, 224)\n",
        "x = torch.randn(1, 3, 224, 224)\n",
        "\n",
        "# pass the input tensor through the model to get the output\n",
        "y = model(x)\n",
        "\n",
        "# create a visualization of the computation graph using graphviz\n",
        "# this helps visualize the flow of information through the model\n",
        "dot = make_dot(y, params=dict(model.named_parameters()))\n",
        "\n",
        "# Save the computation graph as an image\n",
        "dot.render(filename='MyModel', format='png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8pWJHMKjgFB"
      },
      "source": [
        "## **Instantiate the Neural Network**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59QfjTTkhQgS"
      },
      "source": [
        "Here we decide which loss function (Cross Entropy) to use as well as the optimizer (Stochastic Gradient Descent).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9fPnN7uEFSW"
      },
      "outputs": [],
      "source": [
        "# Set the GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Set some parameters\n",
        "epoch = 50\n",
        "learning_rate = 0.1\n",
        "momentum=0.9\n",
        "weight_decay=5e-4\n",
        "\n",
        "# Set the model we are going to use\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "# Loss funtion\n",
        "cost_function = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer = optim.SGD(model_ft.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdsRd1sfrpww"
      },
      "source": [
        "## **Early Stopping**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XN2dg_kjLot"
      },
      "source": [
        "Early stopping is a technique used to prevent overfitting and it is based on the idea that the model's performance on the validation set will stop improving after a certain number of training epochs, and continuing to train beyond this point will only lead to overfitting.\n",
        "\n",
        "In this case the monitor is on the perplexity: after three times we see the perplexity increasing we stop the training and get the final value.\n",
        "\n",
        "There are several benefits to using early stopping:\n",
        "\n",
        "\n",
        "\n",
        "1.   It can help prevent overfitting by stopping the training process before the model starts to overfit the training data;\n",
        "2.   It can save time and resources by avoiding the need to train the model for a large number of epochs;\n",
        "3.   It can improve the generalization ability of the model by avoiding the use of models that are overfitted to the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZFwg-yEvk02"
      },
      "outputs": [],
      "source": [
        "class EarlyStopping:\n",
        " \n",
        "  \"\"\"\n",
        "    A class for early stopping of training process. \n",
        "    \n",
        "    Args:\n",
        "        patience (int): Number of epochs to wait before triggering early stopping. Default: 5\n",
        "        verbose (bool): Flag indicating whether to print messages when early stopping is triggered. Default: True\n",
        "        \n",
        "    Attributes:\n",
        "        patience (int): Number of epochs to wait before triggering early stopping.\n",
        "        verbose (bool): Flag indicating whether to print messages when early stopping is triggered.\n",
        "        counter (int): Counter for the number of epochs since the last improvement.\n",
        "        best_score (float): Best score seen so far. None by default.\n",
        "  \"\"\"\n",
        "  \n",
        "  def __init__(self, patience=5, verbose=True):\n",
        "   \n",
        "    # set the number of epochs to wait before triggering early stopping\n",
        "    self.patience = patience\n",
        "   \n",
        "    # set a flag indicating whether to print messages when early stopping is triggered\n",
        "    self.verbose = verbose\n",
        "   \n",
        "    # initialize the counter for the number of epochs since the last improvement\n",
        "    self.counter = 0\n",
        "   \n",
        "    # initialize the best score seen so far to None\n",
        "    self.best_score = None\n",
        "\n",
        "  def step(self, val_loss):\n",
        "  \n",
        "    \"\"\"\n",
        "        Function to check if early stopping should be triggered.\n",
        "        \n",
        "        Args:\n",
        "            val_loss (float): Current validation loss.\n",
        "        \n",
        "        Returns:\n",
        "            bool: Whether early stopping should be triggered or not.\n",
        "    \"\"\"\n",
        "   \n",
        "    # if this is the first epoch, set the best score to the current validation loss\n",
        "    if self.best_score is None:\n",
        "   \n",
        "      self.best_score = val_loss\n",
        "   \n",
        "    # if the current validation loss is worse than the best score seen so far\n",
        "    elif val_loss > self.best_score:\n",
        "   \n",
        "      # increment the counter\n",
        "      self.counter += 1\n",
        "   \n",
        "      # if the counter has reached the patience threshold\n",
        "      if self.counter >= self.patience:\n",
        "   \n",
        "        # if verbose is True, print a message indicating that early stopping has been triggered\n",
        "        if self.verbose:\n",
        "   \n",
        "          print(f'Early stopping triggered with counter {self.counter} and patience {self.patience}')\n",
        "   \n",
        "        # return True to indicate that early stopping should be triggered\n",
        "        return True\n",
        "   \n",
        "    # if the current validation loss is better than the best score seen so far\n",
        "   \n",
        "    else:\n",
        "   \n",
        "      # set the best score to the current validation loss\n",
        "      self.best_score = val_loss\n",
        "   \n",
        "      # reset the counter\n",
        "      self.counter = 0\n",
        "   \n",
        "    # if the counter has not reached the patience threshold, return False\n",
        "    return False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "an-lhQ7sranJ"
      },
      "source": [
        "## **Training Step**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPvD5OnbpirT"
      },
      "source": [
        "The training_step function is a Python function that represents a training step (training step) of a neural network. The function takes as input a neural network (net), a training set (train_loader), a validation set (valid_loader), an optimizer (optimizer), a cost function (cost_function), and a device (device) on which to run the training (by default \"cuda,\" indicating an NVIDIA GPU).\n",
        "\n",
        "The function under consideration is responsible for training and validating a neural network. To begin with, the function sets the network to training mode by calling the net.train() function. The training cycle is then initiated on the training set using a for loop that iterates over the batches of inputs and targets in the train_loader. For each batch, the function performs the following steps:\n",
        "\n",
        "Firstly, the input image is resized to a specific size. Then, both inputs and targets are moved to the specified device (device). The gradients of the optimizer are then zeroed using the optimizer.zero_grad() function. Next, a forward pass is performed on the neural network, where the outputs are computed by passing the inputs through the network (outputs = net(inputs)). The cost function is then calculated by comparing the outputs with the corresponding targets (loss = cost_function(outputs, targets)). Subsequently, the backward pass is computed to calculate the gradients using loss.backward(). Finally, the optimizer is updated by taking a step in the opposite direction of the gradients using the optimizer.step() function. Additionally, the function calculates the training accuracy (train_acc) and tracks the training loss (train_loss).\n",
        "\n",
        "Upon completing the training cycle on the training set, the function proceeds to the validation cycle on the validation set. A for loop is used to iterate over the batches of inputs and targets in the valid_loader. For each batch, the function repeats steps 1 to 5 and computes the validation accuracy (valid_acc) and tracks the validation loss (valid_loss).\n",
        "\n",
        "At the end of the function, the average training loss (train_loss), average training accuracy (train_acc), average validation loss (valid_loss), and average validation accuracy (valid_acc) are returned as a tuple. These values are useful for monitoring and evaluating the performance of the neural network during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBIAb8RQEHPW"
      },
      "outputs": [],
      "source": [
        "def training_step(net, train_loader, valid_loader, optimizer, cost_function, device='cuda'):\n",
        "\n",
        "    # set the network to training mode\n",
        "    net.train()\n",
        "\n",
        "    # train loop\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    num_train_samples = 0\n",
        "\n",
        "    # iterate over training data\n",
        "    for inputs, targets in train_loader:\n",
        "\n",
        "        # resize the inputs\n",
        "        inputs = Resize((224, 224))(inputs)\n",
        "\n",
        "        # move inputs and targets to specified device\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # zero the optimizer gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward pass\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        # compute the loss\n",
        "        loss = cost_function(outputs, targets)\n",
        "\n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # update model parameters\n",
        "        optimizer.step()\n",
        "\n",
        "        # compute training accuracy\n",
        "        _, predicted = outputs.max(1)\n",
        "        num_train_samples += targets.size(0)\n",
        "        train_acc += predicted.eq(targets).sum().item()\n",
        "\n",
        "        # track training loss\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    # compute average training loss and accuracy\n",
        "    train_loss /= len(train_loader)\n",
        "    train_acc /= num_train_samples\n",
        "\n",
        "    # Validation loop\n",
        "    net.eval()\n",
        "    valid_loss = 0.0\n",
        "    valid_acc = 0.0\n",
        "    num_valid_samples = 0\n",
        "\n",
        "    # iterate over validation data\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in valid_loader:\n",
        "\n",
        "            # resize the inputs\n",
        "            inputs = Resize((224, 224))(inputs)\n",
        "\n",
        "            # move inputs and targets to specified\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            outputs = net(inputs)\n",
        "            loss = cost_function(outputs, targets)\n",
        "\n",
        "            # Compute validation accuracy\n",
        "            _, predicted = outputs.max(1)\n",
        "            num_valid_samples += targets.size(0)\n",
        "            valid_acc += predicted.eq(targets).sum().item()\n",
        "\n",
        "            valid_loss += loss.item()\n",
        "\n",
        "    # calculate average validation loss and accuracy\n",
        "    valid_loss /= len(valid_loader)\n",
        "    valid_acc /= num_valid_samples\n",
        "\n",
        "    return train_loss, train_acc, valid_loss, valid_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-AY3p775rfQd"
      },
      "source": [
        "## **Test Step**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSwZuLplrk5T"
      },
      "source": [
        "The Python function test_step is an important tool for evaluating the performance of a neural network during the validation or testing phase. The function takes as input the neural network (net), a data loader (data_loader), a cost function (cost_function), and a device (device) on which to run the test.\n",
        "\n",
        "The function begins by setting the neural network to evaluation mode (net.eval()). Next, the function iterates through the data loader, which contains the test data set, using a for loop. For each batch of inputs and targets in the data loader, the function performs a series of steps.\n",
        "\n",
        "First, the function moves the inputs and targets to the specified device (device). Then, the function performs a forward pass of the neural network to generate the outputs (outputs = net(inputs)). The function then calculates the cost function (loss = cost_function(outputs, targets)).\n",
        "\n",
        "The evaluation metrics are then updated, which include calculating the total accuracy (total_acc) by adding the total number of samples in the batch (targets.size(0)) to the variable total_acc, and calculating the correct accuracy (correct_acc) by adding the number of correct predictions ((predicted == targets).sum().item()) to the correct_acc variable. The function also calculates the total loss (loss_acc) by summing the batch loss (loss.item()) multiplied by the total number of batch samples (targets.size(0)) to the variable loss_acc.\n",
        "\n",
        "At the end of the function, the average test loss (loss_acc / total_acc) and average test accuracy (correct_acc / total_acc) are returned as a tuple. These values can be used to evaluate the performance of the neural network on the test data. Overall, the test_step function is an essential tool for assessing the accuracy and effectiveness of a neural network, enabling users to make informed decisions regarding model optimization and refinement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0MQpmfgEI_V"
      },
      "outputs": [],
      "source": [
        "def test_step(net, data_loader, cost_function, device):\n",
        "  net.eval()\n",
        "  loss_acc = 0\n",
        "  correct_acc = 0\n",
        "  total_acc = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for inputs, targets in data_loader:\n",
        "      \n",
        "      # load data into GPU\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "\n",
        "      # forward pass\n",
        "      outputs = net(inputs)\n",
        "      loss = cost_function(outputs, targets)\n",
        "\n",
        "      # update metrics\n",
        "      _, predicted = torch.max(outputs.data, 1)\n",
        "      total_acc += targets.size(0)\n",
        "      correct_acc += (predicted == targets).sum().item()\n",
        "      loss_acc += loss.item() * targets.size(0)\n",
        "\n",
        "  return loss_acc / total_acc, correct_acc / total_acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1vcocWG2cb7"
      },
      "source": [
        "The provided code is a Python function called \"main\" that facilitates an entire iteration of a neural network training process for classification. The function requires training configuration parameters, including the number of epochs, batch size, learning rate, weight decay, momentum, training data loader, validation data loader, test data loader, optimizer, and cost function.\n",
        "\n",
        "Within the function, an early stopping criterion is established to terminate training when the validation loss does not improve for a specified number of epochs. The function then iterates through the specified number of epochs and calls the training_step function at each iteration to train the network based on the training and validation data loader.\n",
        "\n",
        "During training, the model parameters with the lowest validation loss up to that point are saved. After training, the function uses the saved model parameters to perform the final testing on the test data.\n",
        "\n",
        "Finally, the function returns the losses and accuracies for the training, validation, and test data as output. Additionally, the code provides an option to save the trained model to disk for future use.\n",
        "\n",
        "This code demonstrates the importance of an iterative approach to training a neural network. By breaking the training process into smaller steps, such as training and validation, the model can be evaluated and refined regularly. The use of early stopping prevents the model from overfitting to the training data, and the ability to save the trained model enables it to be used in other contexts or applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToHU7h0cELNG"
      },
      "outputs": [],
      "source": [
        "def main(batch_size, \n",
        "         device, \n",
        "         learning_rate, \n",
        "         weight_decay, \n",
        "         momentum, \n",
        "         epochs,\n",
        "         train_loader,\n",
        "         valid_loader,\n",
        "         test_loader,\n",
        "         optimizer,\n",
        "         cost_function):\n",
        "\n",
        "    # Create the early stopping criterion\n",
        "    criterion = EarlyStopping(patience=5, verbose=True)\n",
        "\n",
        "    # initialize variables to keep track of best validation loss and corresponding model parameters\n",
        "    best_loss = float('inf')\n",
        "    best_params = None\n",
        "\n",
        "    # iterate over the number of epochs\n",
        "    for e in range(epochs):\n",
        "\n",
        "        # train & log\n",
        "        train_loss, train_acc, valid_loss, valid_acc = training_step(model_ft, train_loader, valid_loader, optimizer, cost_function, device)\n",
        "\n",
        "        # check if the current validation loss is the best seen so far\n",
        "        if valid_loss < best_loss:\n",
        "            best_loss = valid_loss\n",
        "            best_params = model_ft.state_dict()\n",
        "            \n",
        "            # Save the model\n",
        "            torch.save({'model_state_dict': model_ft.state_dict()},\"/content/drive/MyDrive/cvmodels/\"+ model_name + \"_{:.3f}\".format(best_loss))\n",
        "\n",
        "            print(\"Model saved at BaseModel_{:.3f}\".format(best_loss))\n",
        "            \n",
        "        # check if early stopping should be triggered\n",
        "        if criterion.step(valid_loss):\n",
        "            print('Early stopping triggered')\n",
        "            break\n",
        "\n",
        "        ltrain_loss.append(train_loss)\n",
        "        lvalid_loss.append(valid_loss)\n",
        "\n",
        "        # log epoch results\n",
        "        print(f'Epoch {e+1}/{epochs}, Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}, Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_acc:.4f}')\n",
        "\n",
        "    # load the best model parameters\n",
        "    model_ft.load_state_dict(best_params)\n",
        " \n",
        "    # test the model on the test set\n",
        "    test_loss, test_acc = test_step(model_ft, test_loader, cost_function, device)\n",
        "\n",
        "    # log test results\n",
        "    print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}')\n",
        "    return (train_loss,train_acc,valid_loss,valid_acc,test_loss,test_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmrDhGWX2gz9"
      },
      "source": [
        "## **Launch everything**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oknY_IIGE34J"
      },
      "outputs": [],
      "source": [
        "ltrain_loss = []\n",
        "lvalid_loss = []\n",
        "train_loss,train_acc,valid_loss,valid_acc,test_loss,test_acc=main(batch_size, device, learning_rate, weight_decay, momentum, epoch, train_loader, valid_loader, test_loader, optimizer, cost_function)\n",
        "\n",
        "# Set the number of epochs\n",
        "epochs = range(1,len(lvalid_loss)+1)\n",
        "\n",
        "# Create a new figure with white background and set its title and axis labels\n",
        "plt.figure(num=3, figsize=(8, 5)).patch.set_facecolor('pink')\n",
        "plt.title('Regolarized: Train and Valid Losses')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "\n",
        "# Plot the train loss and valid loss for the given number of epochs\n",
        "plt.plot(epochs, ltrain_loss, color='blue', label='Train loss')\n",
        "plt.plot(epochs, lvalid_loss, color='orange', label='Valid loss')\n",
        "\n",
        "\n",
        "# Add a legend to the plot and display it\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## **Initialize the model for this run**"
      ],
      "metadata": {
        "id": "HSmIzHDKsw5S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2RQtk5r8fGFX"
      },
      "outputs": [],
      "source": [
        "class StopExecution(Exception):\n",
        "    def _render_traceback_(self):\n",
        "        pass\n",
        "\n",
        "# set the name of the model to be loaded (resnet, alexnet, vgg, squeezenet, densenet,ResNet18)\n",
        "model_name = \"resnet\"\n",
        "\n",
        "# find the most recent saved model file with the given model name\n",
        "model_file = glob.glob(\"/content/drive/MyDrive/cvmodels/\"+ model_name+\"_*\")\n",
        "print(model_file)\n",
        "nfile = len(model_file)\n",
        "print(nfile)\n",
        "\n",
        "# if no model file is found, print a message and exit\n",
        "if nfile==0 :\n",
        "  print('no model found')\n",
        "  raise StopExecution\n",
        "\n",
        "# otherwise, sort the list of model files and select the most recent one\n",
        "else:\n",
        "  model_file.sort()\n",
        "  path = model_file[nfile-1]\n",
        "  print(path)\n",
        "\n",
        "# initialize the model with the specified architecture and other parameters\n",
        "test_model, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n",
        "\n",
        "# load the saved model parameters from the selected file\n",
        "test_model.load_state_dict(torch.load(path)[\"model_state_dict\"])\n",
        "\n",
        "# move the model to the device (e.g. CPU or GPU) for computation\n",
        "test_model = test_model.to(device)\n",
        "\n",
        "# print the model architecture and parameters for reference\n",
        "print(test_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MrqAr0p3qmX"
      },
      "source": [
        "## **Accuracies**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKD4GVubkORQ"
      },
      "source": [
        "### **Get the predictions from the trained model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZ4sBRcmkUay"
      },
      "source": [
        "This function takes as input the neural network model, the data loader (data_loader) which provides the input data to the model, and the device to run the computation on. It returns the predicted class labels and true class labels as two lists (y_pred and y_true, respectively)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FSEJt475EOvY"
      },
      "outputs": [],
      "source": [
        "def predict(net, data_loader, device='cuda'):\n",
        "    \n",
        "    # set the network to evaluation mode\n",
        "    net.eval()\n",
        "    \n",
        "    # initialize empty lists to store predicted and true labels\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "    features = []\n",
        "\n",
        "    correct_printed = False\n",
        "    incorrect_printed = False\n",
        "\n",
        "    # disable gradient computation (we are only predicting, we do not want our model to be modified in this step!)\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        # iterate over the data loader\n",
        "        for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "    \n",
        "            # load data into GPU\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "\n",
        "            # forward pass\n",
        "            outputs = net(inputs)\n",
        "            #outputs, feats = net(inputs, return_feats=True)\n",
        "\n",
        "            # retrieve the predicted class labels\n",
        "            _, predicted = outputs.max(dim=1)\n",
        "\n",
        "            # append predicted and true labels to their corresponding lists\n",
        "            y_pred += predicted.cpu().numpy().tolist()\n",
        "            y_true += targets.cpu().numpy().tolist()\n",
        "            # features.append(feats.cpu().numpy())\n",
        "\n",
        "\n",
        "            # Print sample images which are correctly and incorrectly classified, along with their paths\n",
        "            # invTrans = transforms.Compose([transforms.Normalize(mean=[-0.5/0.5, -0.5/0.5, -0.5/0.5], std=[1/0.5, 1/0.5, 1/0.5])])\n",
        "            for i, pred in enumerate(y_pred):\n",
        "              if pred == y_true[i] and not correct_printed:\n",
        "\n",
        "                # Find the file name\n",
        "                index = data_loader.dataset.indices[i]\n",
        "                file_name, label = dataset.samples[index]\n",
        "\n",
        "                # Show the image\n",
        "                print(f\"Correct image: {file_name}\")\n",
        "                print(f\"Predicted label: {classes[pred]}({pred}), actual label: {classes[y_true[i]]}({y_true[i]})\")\n",
        "                import cv2\n",
        "                from google.colab.patches import cv2_imshow\n",
        "                img = cv2.imread(file_name, cv2.IMREAD_UNCHANGED)\n",
        "                cv2_imshow(img)\n",
        "\n",
        "                correct_printed = True\n",
        "              elif pred != y_true[i] and not incorrect_printed:\n",
        "\n",
        "                # Find the file name\n",
        "                index = data_loader.dataset.indices[i]\n",
        "                file_name, label = dataset.samples[index]\n",
        "\n",
        "                # Show the image\n",
        "                print(f\"Incorrect image: {file_name}\")\n",
        "                print(f\"Predicted label: {classes[pred]}({pred}), actual label: {classes[y_true[i]]}({y_true[i]})\")\n",
        "                import cv2\n",
        "                from google.colab.patches import cv2_imshow\n",
        "                img = cv2.imread(file_name, cv2.IMREAD_UNCHANGED)\n",
        "                cv2_imshow(img)\n",
        "\n",
        "                incorrect_printed = True\n",
        " \n",
        "            \n",
        "    # return the predicted and true labels as lists\n",
        "    return y_pred, y_true\n",
        "\n",
        "y_true, y_pred = predict(test_model, test_loader, device=\"cuda\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **ROC Curve**"
      ],
      "metadata": {
        "id": "QActGkFt7zq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "y_pred_roc = to_categorical(y_pred, num_classes=12)\n",
        "y_true_roc = to_categorical(y_true, num_classes=12)"
      ],
      "metadata": {
        "id": "Sc_0wAeVIB81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# assuming y_true and y_pred are the true and predicted labels, respectively\n",
        "y_pred_roc = np.array(y_pred_roc)\n",
        "y_true_roc = np.array(y_true_roc)\n",
        "\n",
        "# binarize the multiclass labels\n",
        "y_true_roc = label_binarize(y_true_roc, classes=np.arange(12))\n",
        "\n",
        "# initialize variables to store fpr, tpr, and roc_auc for all classes\n",
        "all_fpr = dict()\n",
        "all_tpr = dict()\n",
        "all_roc_auc = dict()\n",
        "\n",
        "# calculate fpr, tpr, and roc_auc for each class\n",
        "for i in range(12):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true_roc[:, i], y_pred_roc[:, i])\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "    all_fpr[i] = fpr\n",
        "    all_tpr[i] = tpr\n",
        "    all_roc_auc[i] = roc_auc\n",
        "\n",
        "# plot ROC curves for all classes\n",
        "for i in range(12):\n",
        "    plt.plot(all_fpr[i], all_tpr[i], label='ROC curve for class %d (area = %0.2f)' % (i, all_roc_auc[i]))\n",
        "    \n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic curve for all classes')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wlK-BpESIvHB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sZoLtvYki_U"
      },
      "source": [
        "### **Confusion Matrix**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FX9D92MpfbE"
      },
      "source": [
        "A confusion matrix is a table that is used to evaluate the performance of a classification model. It is a way to visualize the number of true positives, false positives, true negatives, and false negatives that are produced by a model's predictions.\n",
        "\n",
        "The confusion matrix has two dimensions: the actual values and the predicted values. The actual values are the true labels of the data, while the predicted values are the labels that the model has assigned to the data.\n",
        "\n",
        "The four components of the confusion matrix are:\n",
        "*   True positives (TP): The number of positive instances that were correctly classified as positive by the model;\n",
        "*   False positives (FP): The number of negative instances that were incorrectly classified as positive by the model\n",
        "*   True negatives (TN): The number of negative instances that were correctly classified as negative by the model;\n",
        "*   False negatives (FN): The number of positive instances that were incorrectly classified as negative by the model.\n",
        "\n",
        "\n",
        "The confusion matrix provides a way to evaluate how well a classification model is performing, and it is often used to calculate other evaluation metrics such as accuracy, precision, recall, and F1-score. By examining the confusion matrix, you can get a better understanding of the strengths and weaknesses of your model and adjust it accordingly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8f9l6dJEQfE"
      },
      "outputs": [],
      "source": [
        "def confusion_matrix(y_true, y_pred, num_classes):\n",
        "    \n",
        "    # Create an empty confusion matrix with num_classes rows and columns\n",
        "    conf_mat = np.zeros((num_classes, num_classes))\n",
        "    \n",
        "    # Loop over each example in the dataset\n",
        "    for i in range(len(y_true)):\n",
        "        \n",
        "        # Get the true class and predicted class for the current example\n",
        "        true_class = int(y_true[i])\n",
        "        pred_class = int(y_pred[i])\n",
        "        \n",
        "        # Increment the corresponding entry in the confusion matrix\n",
        "        conf_mat[true_class, pred_class] += 1\n",
        "    \n",
        "    # Return the resulting confusion matrix\n",
        "    return conf_mat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pJvKDqVqg9M"
      },
      "source": [
        "Let's plot here the confusion matrix we have computed above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dAEzAxVdETXE"
      },
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(y_true, y_pred, classes):\n",
        "    # Compute confusion matrix\n",
        "    num_classes=len(classes)\n",
        "    conf_mat = confusion_matrix(y_true, y_pred, num_classes)\n",
        "    \n",
        "    # Create heatmap\n",
        "    fig, ax = plt.subplots(figsize=(10, 10))\n",
        "    sns.heatmap(conf_mat, annot=True, fmt='.2f', cmap='Blues', square=True, cbar=False, ax=ax)\n",
        "    \n",
        "    # Set labels and ticks\n",
        "    ax.set_xlabel('Predicted Labels')\n",
        "    ax.set_ylabel('True Labels')\n",
        "    ax.set_xticklabels(classes, rotation=90)\n",
        "    ax.set_yticklabels(classes, rotation=0)\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "\n",
        "plot_confusion_matrix(y_true, y_pred, classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kxuTnugrOUR"
      },
      "source": [
        "### **Calculate accuracy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__Qz1HQ3sLP8"
      },
      "source": [
        "Accuracy is a common evaluation metric for classification models that measures the proportion of correct predictions among all predictions made by the model. In this function, the accuracy is calculated by dividing the number of correct predictions by the total number of examples.\n",
        "\n",
        "The correct variable in the function keeps track of the number of examples for which the predicted label matches the true label. The total variable keeps track of the total number of examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ve8QVC604R8x"
      },
      "outputs": [],
      "source": [
        "def calculate_accuracy(model, dataloader, device):\n",
        "    \n",
        "    # Put the model in evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    # Initialize variables to keep track of the number of correct predictions and total examples\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    # Use torch.no_grad() to disable gradient calculation during evaluation\n",
        "    with torch.no_grad():\n",
        "        \n",
        "        # Loop over each batch of data in the dataloader\n",
        "        for data in dataloader:\n",
        "            \n",
        "            # Move the input data and labels to the specified device\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "            \n",
        "            # Make predictions using the model\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, dim=1)\n",
        "            \n",
        "            # Update the total count of examples and number of correct predictions\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    \n",
        "    # Calculate the accuracy by dividing the number of correct predictions by the total number of examples\n",
        "    accuracy = correct / total\n",
        "    \n",
        "    # Return the accuracy\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cjl9RVuzsC4S"
      },
      "source": [
        "Let's print the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_sr8i0u94UEz"
      },
      "outputs": [],
      "source": [
        "accuracy = calculate_accuracy(test_model, test_loader, \"cuda\")\n",
        "print(\"Accuracy:\", accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXZ_YYQRsImU"
      },
      "source": [
        "### **Weighted Accuracy**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jEBV2AzOztk7"
      },
      "source": [
        "Weighted accuracy is a metric for classification models that measures the average accuracy weighted by the number of samples in each class. The idea behind this metric is to give higher weight to the accuracy of classes with more samples, as misclassifying a sample in a larger class has a greater impact on the overall accuracy of the model than misclassifying a sample in a smaller class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_inbBHIJ4gM3"
      },
      "outputs": [],
      "source": [
        "def calculate_weighted_accuracy(model, data_loader, num_classes):\n",
        "    \n",
        "    # Get the device the model is running on\n",
        "    device = next(model.parameters()).device\n",
        "    \n",
        "    # Initialize lists to keep track of correct and total predictions for each class\n",
        "    class_correct = [0.0] * num_classes\n",
        "    class_total = [0.0] * num_classes\n",
        "    \n",
        "    # Disable gradient computation for evaluation\n",
        "    with torch.no_grad():\n",
        "        \n",
        "        # Loop over each batch of data in the data loader\n",
        "        for data in data_loader:\n",
        "            \n",
        "            # Move the input data and labels to the specified device\n",
        "            inputs, labels = data\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "            \n",
        "            # Make predictions using the model\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            \n",
        "            # Update the number of correct and total predictions for each class\n",
        "            c = (predicted == labels).squeeze()\n",
        "            for i in range(labels.size(0)):\n",
        "                label = labels[i]\n",
        "\n",
        "                try:  # Tensor.squeeze() could squeeze in such a way that tensor only has one item instead of a list, so c[i] won't work\n",
        "                  class_correct[label] += c[i].item()\n",
        "                except Exception:\n",
        "                  class_correct[label] += c.item()\n",
        "\n",
        "                class_total[label] += 1\n",
        "\n",
        "    # Calculate the weight of each class as its proportion of the total number of examples\n",
        "    class_weight = [class_total[i] / sum(class_total) for i in range(num_classes)]\n",
        "    \n",
        "    # Calculate the weighted accuracy as the weighted average of accuracy for each class\n",
        "    weighted_accuracy = sum([class_weight[i] * class_correct[i] / class_total[i] for i in range(num_classes)])\n",
        "    \n",
        "    # Return the weighted accuracy\n",
        "    return weighted_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pwz0QpJ5zva4"
      },
      "source": [
        "Let's print the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CQGCZ2GI55U8"
      },
      "outputs": [],
      "source": [
        "weighted_accuracy = calculate_weighted_accuracy(test_model, test_loader,12)\n",
        "print(\"Weighted accuracy:\", weighted_accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F86cfUd008Xf"
      },
      "source": [
        "### **Precision**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wxq9Wdpz0-iw"
      },
      "source": [
        "Precision is a common performance metric used in machine learning and statistics to evaluate the accuracy of a model or classifier.\n",
        "\n",
        "Precision is the fraction of true positive predictions out of all positive predictions made by the model. In other words, precision measures how many of the positive predictions made by the model are actually correct.\n",
        "\n",
        "Here's the formula for precision:\n",
        "\n",
        "Precision = true positives / (true positives + false positives)\n",
        "\n",
        "Where:\n",
        "\n",
        "*  True positives (TP) are the number of correct positive predictions made by the model;\n",
        "*  False positives (FP) are the number of incorrect positive predictions made by the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHJDo8cR_bx8"
      },
      "outputs": [],
      "source": [
        "def calculate_precision(model, data_loader):\n",
        "    # Set the model to evaluation mode\n",
        "    model.eval()\n",
        "    # Initialize lists to store the predicted and true labels\n",
        "    predictions = []\n",
        "    targets = []\n",
        "    # Iterate through the data loader\n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            # Move the data to the device\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            # Forward pass through the model\n",
        "            outputs = model(images)\n",
        "            # Get the predicted class labels\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            # Move the predicted labels to the CPU\n",
        "            predicted = predicted.to('cpu')\n",
        "            # Append the true and predicted labels to the lists\n",
        "            targets.append(labels.to('cpu'))\n",
        "            predictions.append(predicted)\n",
        "    # Concatenate the predicted and true labels into tensors\n",
        "    predictions = torch.cat(predictions)\n",
        "    targets = torch.cat(targets)\n",
        "    # Compute the weighted average precision score\n",
        "    precision = precision_score(targets, predictions, average='weighted')\n",
        "    return precision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOXQSJ-B5S2i"
      },
      "source": [
        "Let's print the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qqN7LI-_d_l"
      },
      "outputs": [],
      "source": [
        "precision = calculate_precision(model_ft, test_loader)\n",
        "print(\"Precision:\", precision)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rcme5zDY1S35"
      },
      "source": [
        "### **Recall**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jmpi6ID1UoL"
      },
      "source": [
        "Recall is a common metric used in machine learning to evaluate the performance of a binary classifier. It measures the proportion of true positive predictions made by the model out of all actual positive instances in the dataset.\n",
        "\n",
        "Recall is calculated as the ratio of the number of true positive predictions to the sum of true positive and false negative predictions. In other words:\n",
        "\n",
        "$Recall = True Positives / (True Positives + False Negatives)$\n",
        "\n",
        "A high recall score indicates that the model is good at correctly identifying all instances of the positive class, but it may also result in a higher number of false positive predictions. On the other hand, a low recall score indicates that the model is not good at correctly identifying positive instances, which can be problematic if the positive class is of high importance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FDTulXTAncj"
      },
      "outputs": [],
      "source": [
        "def calculate_recall(model, data_loader):\n",
        "    \n",
        "    \"\"\"\n",
        "    Calculates the recall score of a PyTorch model on a dataset.\n",
        "\n",
        "    Args:\n",
        "        model (nn.Module): The PyTorch model to evaluate.\n",
        "        data_loader (DataLoader): A PyTorch DataLoader object containing the evaluation dataset.\n",
        "\n",
        "    Returns:\n",
        "        float: The weighted recall score of the model on the dataset.\n",
        "    \"\"\"\n",
        "\n",
        "    # Switch the model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Create empty lists to store the true and predicted labels\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    # Disable gradient calculations to save memory\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # Iterate over the evaluation dataset\n",
        "        for inputs, labels in data_loader:\n",
        "\n",
        "            # Move the inputs and labels to the appropriate device (e.g., GPU)\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Make predictions with the model\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Get the predicted labels as the class with the highest output score\n",
        "            _, predicted = torch.max(outputs, dim=1)\n",
        "\n",
        "            # Append the true and predicted labels to the lists\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "    # Calculate the recall score using scikit-learn's recall_score function\n",
        "    return recall_score(y_true, y_pred, average='weighted')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qbEzDsh1VTz"
      },
      "source": [
        "Let's print the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dz-u00_mAn2O"
      },
      "outputs": [],
      "source": [
        "recall = calculate_recall(model_ft, test_loader)\n",
        "print(\"Recall:\", recall)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDd5My8TzyHr"
      },
      "source": [
        "### **F1 Score**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d23BH1tjz0b5"
      },
      "source": [
        "The F1 score is a metric commonly used to evaluate the performance of a classification model. It is the harmonic mean of precision and recall, and ranges between 0 and 1, where 1 indicates perfect precision and recall, and 0 indicates that the model has failed to correctly classify any samples.\n",
        "Precision is the proportion of correctly predicted positive samples out of all the predicted positive samples, while recall is the proportion of correctly predicted positive samples out of all the true positive samples. The F1 score takes into account both precision and recall, and is a good metric to use when the classes are imbalanced, i.e., when some classes have significantly fewer samples than others"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sXU6RZoP9m65"
      },
      "outputs": [],
      "source": [
        "# This function calculates the F1 score of a classification model\n",
        "# The F1 score is a metric that combines precision and recall\n",
        "# It is a harmonic mean of precision and recall, and ranges between 0 and 1\n",
        "\n",
        "def calculate_f1_score(model, data_loader):\n",
        "    model.eval()\n",
        "    \n",
        "    # Initialize empty lists to store true and predicted labels\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for images, labels in data_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            \n",
        "            # Get the predicted labels by finding the index of the maximum log-probability\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            \n",
        "            # Add the true and predicted labels to the respective lists\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "    \n",
        "    # Compute the F1 score using scikit-learn's f1_score function\n",
        "    # The 'weighted' option computes the average F1 score weighted by the number of samples in each class\n",
        "    return f1_score(y_true, y_pred, average='weighted')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dztNS5wb04nM"
      },
      "source": [
        "Let's print the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zECMu8BN9nsM"
      },
      "outputs": [],
      "source": [
        "f1_score=calculate_f1_score(model_ft, test_loader)\n",
        "print(\"f1_score:\", f1_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hK2Gz9Em1XNH"
      },
      "source": [
        "### **K-fold Cross Validation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmXLpvDv1dnX"
      },
      "source": [
        "K-fold cross-validation is a popular method used in machine learning to evaluate the performance of a model on a dataset. It involves dividing the dataset into k equally sized subsets, or \"folds\", and training and testing the model k times. In each iteration, one fold is used as the test set and the remaining k-1 folds are used as the training set.\n",
        "\n",
        "The process of training and testing the model is repeated k times, with each fold used exactly once as the test set. The results from each iteration are then averaged to obtain a final evaluation score for the model.\n",
        "\n",
        "K-fold cross-validation is commonly used in machine learning for several reasons:\n",
        "\n",
        "It allows for a more reliable estimate of the model's performance on unseen data, as all data points are used for both training and testing.\n",
        "It helps to reduce the risk of overfitting, as the model is trained and evaluated on different subsets of the data.\n",
        "It allows for a better use of available data, especially in cases where the dataset is small.\n",
        "One important consideration when using k-fold cross-validation is the choice of k"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_XGYUk0y2hJ"
      },
      "outputs": [],
      "source": [
        "# number of folds\n",
        "k = 5\n",
        "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "valid_loss = []\n",
        "valid_acc = []\n",
        "\n",
        "for fold, (train_indices, val_indices) in enumerate(kf.split(dataset)):\n",
        "    print(f'Fold {fold+1}/{k}')\n",
        "    \n",
        "    # create data loaders for the current fold\n",
        "    train_sampler = SubsetRandomSampler(train_indices)\n",
        "    val_sampler = SubsetRandomSampler(val_indices)\n",
        "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
        "    val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)\n",
        "    \n",
        "    # train the model for the current fold\n",
        "    train_loss, train_acc, val_loss, val_acc = training_step(test_model, train_loader, valid_loader, optimizer, cost_function, device)\n",
        "    \n",
        "    # compute and save the validation performance for the current fold\n",
        "    valid_loss.append(val_loss)\n",
        "    valid_acc.append(val_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rRFSzJXZJiL"
      },
      "outputs": [],
      "source": [
        "print(valid_loss)\n",
        "print(valid_acc)\n",
        "\n",
        "for i in range(k):\n",
        "  print(f\"Fold: {i}\")\n",
        "  print(f\"Loss: {valid_loss[i]}\")\n",
        "  print(f\"Accuracy: {valid_acc[i]}\")\n",
        "\n",
        "avg_acc = round(sum(valid_acc)/k,5)\n",
        "std_list= round(np.std(valid_acc),5)\n",
        "print('Average accuracy: ', avg_acc,'+-', std_list) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lD9AXfRCrHK"
      },
      "source": [
        "### **T-SNE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOni44O1_p8i"
      },
      "source": [
        "We define two functions:\n",
        "\n",
        "*  get_tsne generates the t-SNE regarding the classes of target samples;\n",
        "*  get_tsne_src_tgt generates the t-SNE regarding the source and target samples.\n",
        "\n",
        "Both functions take in input the features extracted from the last layer of the network, before the final linear classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eUdwZqe09T9z"
      },
      "outputs": [],
      "source": [
        "def extract_features(model, dataloader, device):\n",
        "    model.eval()\n",
        "    features = []\n",
        "    labels = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            outputs = model(inputs)\n",
        "            features.append(outputs.cpu().numpy())\n",
        "            labels.append(targets.cpu().numpy())\n",
        "    features = np.concatenate(features, axis=0)\n",
        "    labels = np.concatenate(labels, axis=0)\n",
        "    # get features from the last layer of the model\n",
        "    features = np.squeeze(features)\n",
        "    return features, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pZt5Ao0c73Q"
      },
      "outputs": [],
      "source": [
        "features, labels = extract_features(test_model, test_loader, \"cuda\")\n",
        "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
        "tsne_results = tsne.fit_transform(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JSw8tlH7CvVA"
      },
      "outputs": [],
      "source": [
        "plt.scatter(tsne_results[:,0], tsne_results[:,1], c=labels, cmap='tab10')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "1Tk_00Sbeic4",
        "nvoboEUl199_",
        "LiPw8iATX22p",
        "3na1coO_r7V7",
        "v8EBSIrqgOEC",
        "q8pWJHMKjgFB",
        "DdsRd1sfrpww",
        "an-lhQ7sranJ"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}